\documentclass[11pt,a4,twosided,singlespacing,titlepagenumber=on]{scrreprt}

\usepackage[T1]{fontenc} % Handles accents etc better in the invisible details of the pdf output.
\usepackage[latin1]{inputenc} % May or may not be needed. Says that your *.tex file is a text file with ASCII latin1 encoding. You could use e.g. utf8 instead for easier accents etc.
\usepackage[UKenglish]{babel} % Let LaTeX know what language the text is in so it can select the correct hyphenation pattern etc

$\newcommand{\Var}{\mathrm{Var}}$

%%% American Mathematical Society packages
\usepackage{amsfonts,amssymb,amsmath,amsthm}
\usepackage{amsbsy}


\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

%\usepackage{bm} % Possibly a better alternative to amsbsy for making bold typeface math.

%%% Graphics packages
\usepackage{graphicx}
%\graphicspath{{figures/}} % Useful if you have lots of images and want to keep thinks tidy by having a subfolder for images
%\usepackage{epstopdf} % If you produce your graphs as .eps files but then want to compile straight to PDF (e.g. because you are using TeXworks.) you may want to use this option. A better alternative of course would be to save all your graphs as *.pdf files from the start. Note that if you are compiling to pdf through PS/DVI then all your figures should be *.eps files and the epstopdf package should not be used.
%\usepackage{tikz} %For creating vector-graphics diagrams, flowcharts etc directly in LaTeX (takes some time to learn)
\usepackage[absolute]{textpos} % Used to position the Imperial College logo. You can comment this line and the next line out if you don't use the logo.
%\setlength{\TPHorizModule}{\paperwidth}\setlength{\TPVertModule}{\paperheight}
%\setlength{\TPHorizModule}{1cm}\setlength{\TPVertModule}{1cm}


%%% Referencing and cross-referencing
%\usepackage{color}
%\usepackage[colorlinks=false,linkcolor=red,urlcolor=cyan,citecolor=blue,breaklinks,plainpages=false,pdfpagelabels]{hyperref} % To make the hyperlinked cross-referencing visible.
\usepackage[colorlinks=false,pdfborder={0 0 0},plainpages=false,pdfpagelabels]{hyperref} % If you click on an item in the table of contents or a referenced equation/figure number, the PDF will go to the desired page. Neat isn't it?
\usepackage[round,authoryear,sort]{natbib} % Enable bibtex-based bibliography generation
%\usepackage[square,numbers,sort&compress]{natbib} % If you want numbered referencing instead of author-year style.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Create or control Macros   %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\setcounter{secnumdepth}{3} %If you want subsubsections to be numbered
\numberwithin{equation}{chapter} % Reset equation numbers after each chapter.

%%% Theorem environments
\newtheorem{theorem}{Theorem}%[chapter]
\newtheorem{proposition}[theorem]{Proposition}%[chapter]
\newtheorem{definition}[theorem]{Definition}%[chapter]
\newtheorem{lemma}[theorem]{Lemma}%[chapter]
\newtheorem{corollary}[theorem]{Corollary}%[chapter]
%
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}%[chapter]
\newtheorem{example}[theorem]{Example}%[chapter]

%%% Potentially useful style changes:
%\renewcommand{\titlefont}{\normalcolor \normalfont \bfseries} %Change the title font from sans-serif to serif (the same font used for the rest of the document).
%\renewcommand*{\labelitemi}{$\bullet$} %Bullet points in the itemize environment.
%\renewcommand*{\tilde}{\widetilde} % Wider tildes
%\renewcommand*{\bar}{\overline} % Wider conjugate bars

%%% Examples of commands/macros that could be useful:
%\newcommand{\expectation}[1]{\mathbb{E}\left[ #1 \right]}
%\newcommand*{\setR}{{\mathbb R}}
%\newcommand{\commentify}[1]{} %Gives you an alternative way (other than %) to comment things out.
%%% These commands make it faster to get the correct roman font in equations: (similar to \exp, \cos, \sin etc). Alternatively yuo can always use e.g. $\mathrm{Var}$, but this is better.
%\DeclareMathOperator{\bigo}{O}
%\DeclareMathOperator{\littleo}{o}
%\DeclareMathOperator{\var}{Var}
%\DeclareMathOperator{\cov}{Cov}
%\DeclareMathOperator{\trace}{trace}
%\DeclareMathOperator{\sign}{sgn}
%\DeclareMathOperator{\rank}{rank}
%\DeclareMathOperator{\vecrm}{vec} % The \vec command already exists so you can't name this \vec.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Define how to create the title page  %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\newcommand*{\supervisor}[1]{\gdef\@supervisor{#1}}
\newcommand*{\CID}[1]{\gdef\@CID{#1}}
\newcommand*{\logoimg}[1]{\gdef\@logoimg{#1}}
\renewcommand{\maketitle}{
\begin{titlepage}
\ifdefined\@logoimg
\begin{textblock*}{8cm}(1.75cm,1.75cm)
\includegraphics[width=70mm]{\@logoimg}
\end{textblock*}
\vspace*{1cm}
\else
%\vspace*{0cm}
\fi
\begin{center}
\vspace*{\stretch{0.1}}
Imperial College London\\
Derpartment of Mathematics\par
\vspace*{\stretch{1}} % This inserts vertical space and allows you to specify a relative size for the vertical spaces.
{\titlefont\Huge \@title\par} % If your title is long, you may wish to use \huge instead of \Huge.
\vspace*{\stretch{2}}
{\Large \@author \par}
\vspace*{1em}
{\large CID: \@CID \par}
\vspace*{\stretch{0.5}}
{\large Supervised by \@supervisor \par}
\vspace*{\stretch{3}}
{\Large \@date \par}
\vspace*{\stretch{1}}

\textit{This report is submitted as part requirement for the MSc Degree in Statistics at Imperial College London. It is substantially the result of my own work except where explicitly indicated in the text.
The report will be distributed to the internal and external examiners, but thereafter may not be
copied or distributed except with permission from the author.}
\vspace*{\stretch{0.1}}
\end{center}%
\end{titlepage}%
}
\makeatother


%%% And the abstract page
\renewenvironment{abstract}%
{\chapter*{Abstract}\thispagestyle{plain}}%
{\clearpage}
%%% And why not change the quote environment
\newenvironment{myquote}%
{\begin{quote}{\Large{}``}}%
{\ifhmode\unskip\fi{\Large{}''}\end{quote}}


\title{State Space Modelling for Statistical Arbitrage}
\author{Philippe~Remy}
\CID{00993306}
\supervisor{Nikolas Kantas and Yanis Kiskiras}
\date{\today}
\logoimg{Imperial__4_colour_process.jpg}

\begin{document}


\maketitle

\declaration

\begin{abstract}
This project is aimed to investigate the practical benefit of using more complex modelling than what is currently standard practice in applications related to statistical arbitrage. The underlying assets will be modelled using appropriate mean-reverting time series or state space models. In order to fit these models to real data the project will involve using advanced particle methods such as Particle Markov Chain Monte Carlo. The primary aim of the project is to assess whether using more advanced modelling and model calibration will result to better performance than simple models used often in practise. This will be illustrated in numerical examples, where the computed portfolio is used for a realistic scenario obtained by popular trading platforms. Simulations will be mainly run in Matlab, but embedding C/C++ routines may be required to speed up computations. The project is a challenging computational Statistics application to finance and is this suitable for a student with an interest in finance, very good aptitute to computing and understanding of the material in the course related to Monte Carlo methods and Time Series. \\

Throughout, $p(\cdot)$ and $p(\cdot| \cdot)$ are used to denote general marginal and conditional probability density functions, with the arguments making it clear which distributions these relate to. \\
A realization of a stochastic process $X$ on a finite discrete space $\{0,...,T\}$ is denoted $x_{0:T} = (x_1,...,x_T)$.
\end{abstract}
\newpage
%\chapter*{Acknowledgements}
%Thank you supervisor/friends/family/pet.
%\begin{myquote}
%Include an acknowledgement.
%\end{myquote}
%\newpage

% Automatically create a table of contents
%\renewcommand{\contentsname}{Table of Contents}
%\tableofcontents
%\newpage

% Figure and table lists if you want them.
%\cleardoublepage
%\phantomsection
%\listoffigures 
%\addcontentsline{toc}{chapter}{\listfigurename}
%\newpage
%\phantomsection
%\listoftables  
%\addcontentsline{toc}{chapter}{\listtablename}
%\newpage


%\pagestyle{headings} % uncomment this if you want headers on your pages. Google fancyhdr or look into the options of scrreprt if you want different headers.


\chapter{Cointegration}

\section{Theory}
Cointegration is a statistical property possessed by some time series based on the concepts of stationary and the order of integration of the series. A series is considered stationary if its distribution is time invariant. In other words, the series will constantly return to its time invariant mean value as fluctuations occur. In contrast, a non-stationary series will exhibit a time varying mean. A series is said to be integrated of order $d$, denoted $I(d)$ if it must be differenced at least $d$ times to produce a stationary series.
Charles Nelson and Charles Plosser (1982) showed that most time series have stochastic trends and are $I(1)$. \\

The significance of cointegration analysis is its intuitive appeal for dealing with difficulties that arise when using non-stationary series, particularly those that are asumed to have a long-tun equilibrum relationship. For instance, when non-stationary series are used in regression analysis, one as a dependent variable and the others as independent variables, statistical inference becomes problematic. Assume that $y_t$ and $x_t$ be two independent random walk for every $t$, and let's consider the regression : $y_t = a x_t + b + \epsilon_t$. It is obvious that the true value of $a$ is 0 because $cor(x_t, y_t) = 0$. But the limiting distribution of $\hat{a}$ is such that $\hat{a}$ converges to a function of Brownian motions. This is called a spurious regression, and was first noted by Monte Carlo studies by Granger and Newbold (1974). If $x_t$ and $y_t$ are both unit root processes, classical statistical applies for the regression : $\Delta y_t = b + a \Delta x_t + \epsilon_t$ since both are stationary variables. $\hat{a}$ is now a standard consistent estimator. Recall $\Delta$ is the first difference operator defined by : $\Delta x_t = x_t - x_{t-1}$. \\

Cointegration is said to exist between two or more non-stationary time series if they possess the same order of integration and a linear combination of these series is stationary. Let $X_t = (x_{1t},...,x_{nt})_{t \geq 0}$ be $n$ $I(1)$ processes. The vector $(X_t)_{t \geq 0}$ is said to be cointegrated if there exists at least one non trivial vector $\beta = (\beta_1,...,\beta_n)$ such that $\epsilon_t = \beta^T X_t$ is a stationary process $I(0)$. $\beta$ is called a cointegration vector, then so is $k \beta$ for any $k \neq 0$ since $k\beta^TX_t \sim I(0)$. There can be $r$ different cointegrating vector, where $0 \leq r \textless n$, i.e. $r$ must be less than the number of variables $n$. In such a case, we can distinguish between a long-rtun relationship between the variables contained in $X_t$, that is, the manner in which the variables drift upward together, and the short-run dynamics, that is the relationship between deviations of each variable from their correspondinng long-run trend. The implication that non-stationary variables can lead to spurious regressions unless at least one cointegration vector is present means that some form of testing for cointegration is almost mandatory.\\

\section{Vector Auto Regressive Process (VAR)}
The Vector Autoregressive (VAR) process is a generalization of the univariate AR process to the multivariate case. It is defined as:

$$X_t = \nu + \sum_{j=1}^k A_j X_{t-j} + \epsilon_t \text{, } \epsilon_t \sim SWN(0, \Sigma)$$
where $X_t = (x_{1t},...,x_{nt})_{t \geq 0}$, each of the $A_j$ is a ($nxn$) matrix of parameters, $\nu$ is a fixed vector of intercept terms. Finally $\epsilon_t$ is a n-dimensional strict white noise process of covariance matrix $\Sigma$. The process $X_t$ is said to be stable if the roots of the determinant of the characteristic polynomial $|I_n - \sum_{j=1}^k A_j z^j| = 0$ lie outside the complex unit circle.

\section{Vector Error Correction models (VECM)}
In an error correction model, the changes in a variable depend on the deviations from some equilibrium relation. Suppose the case $n=2, X_t = (x_t, y_t)$ where $x_t$ represents the price of a Future contract on a commodity and $y_t$ is the spot price of this same commodity traded on the same market. Assume further more that the equilibrium relation between them is given by $y_t = \beta x_t$ and the increments of $y_t$, $\Delta y_t$ depend on the deviation from this equilibrium at time $t-1$. A similar relation may also hold for $x_t$. The system is defined by:
\begin{align*}
\Delta y_t &= \alpha (y_{t-1} - \beta x_{t-1}) + \epsilon_{y_t} \\
\Delta x_t &= \alpha (y_{t-1} - \beta x_{t-1}) + \epsilon_{x_t}
\end{align}
where $\alpha$ represents the speed of adjustements to disequilibrium and $\beta$ is the long run coefficient of the equilibrium. In a more general error correction model, the $\Delta y_t$ and $\Delta x_t$ may in addition depend on previous changes in both variables as, for instance, in the following model with lag one:
\begin{align*}
\Delta y_t &= \alpha (y_{t-1} - \beta x_{t-1}) + \gamma_{11} \Delta y_{t-1} + \gamma_{12} \Delta x_{t-1} + \epsilon_{y_t} \\
\Delta x_t &= \alpha (y_{t-1} - \beta x_{t-1}) + \gamma_{21} \Delta y_{t-1} + \gamma_{22} \Delta x_{t-1} + \epsilon_{x_t}
\end{align}
In matrix notation and in the general case, the VECM is written as:
$$\Delta y_t = \prod y_{t-1} + \sum_{j=1}^{k-1} \Gamma_j \Delta_{t-j} + \epsilon_t $$
where $\Gamma_j = - \sum_{i=j+1}^k A_i$ and $\prod = - \sum_{i=1}^k A_i$. This way of specifying the system contains information on both the short-run and long run adjustements to changes in $y_t$, via the estimates of $\hat{\Gamma_j}$ and $\hat{\prod}$ respectively. To be continued...

\chapter{Particle MCMC}

\section{Introduction}
Particle MCMC embeds a particle filter within an MCMC scheme. The standard version uses a particle filter to propose new values for the stochastic process (basically $x_{0:T}$), and MCMC moves to propose new values for the parameters (usually named $\theta$). One of the most challenging task in designing a PMCMC sampler is considering the trade-off between the Monte Carlo error of the particle filter and the mixing of the MCMC moves. Intuitively, when $N$, the number of particles grows to infinity, the variance of the error becomes very small and the mixing of the chain becomes very poor.

\section{State-Space Models}
The state-space models are parameterised by $\theta = (\theta_1,...,\theta_n)$ and all components are considered to be independent one another. $\theta$ is associated a prior distribution $p(\theta) = \prod_i p(\theta_i)$ State-space models are usually defined in continuous time because physical laws are most often described in terms of differential equations. However, most of the time, a discrete-time representation exists. It is often written in the innovation form that describes noise. An example of such a process is describe in the Stochastic Volatility section. The model is composed of an unobserved process $X_{0:T}$ and $Y_{1:T}$, known as the observations. $X_{0:T}$ is assumed to be first order markovian, governed by a transition kernel $K(x_{t+1}|x_t)$. The probability density of a realization $x_{0:T}$ is written as:

$$p(X_{0:T} = x_{0:T} | \theta) = p(x_1|\theta) \prod_{t=2}^T p(x_t|x_{t-1}, \theta) $$

The process $X$ is not observed directly, but through $y_{1:T}$. The state-space model assumes that each $y_t$ is dependent of $x_t$. As a consequence, the conditional likelihood of the observations, given the state process can be derived as:

$$p(y_{1:T} | x_{1:T}, \theta) = \prod_{t=1}^T p(y_t | x_t, \theta) $$

The general idea is to find $\theta$ which maximize the marginal likelihood $p(y_{1:T}|\theta)$, $x$ integrated out. It is interesting to begin by the approximation of $p(x_{1:T}, \theta | y_{1:T})$. By Bayes theorem:

\begin{align*}
p(x_{1:T}, \theta | y_{1:T}) &\propto p(\theta)p(x_{1:T}|\theta)p(y_{1:T}|x_{1:T}, \theta) \\
 &= p(\theta) p(x_1|\theta) \prod_{t=2}^T p(x_t|x_{t-1}, \theta) \prod_{t=1}^T p(y_t | x_t, \theta)
\end{align*}

Usually, this probability density function is intractable since it becomes incredibly demanding in resources when $T$ grows. That is where the particle filter comes in.

\section{Particle Filter}

The particle filter is an iterative Monte Carlo method for carrying out bayesian inference on state-space models. The main idea is to assume that, at each time $t$, an approximation of $p(x_t|y_{1:t}$) can help generating approximate samples of $p(x_{t+1}|y_{1:t+1}$), using importance resampling.

More precisely, the procedure is initialised with a sample from $x_0^k \sim p(x_0),\ k=1,\ldots,M$ with uniform normalised weights ${w'}_0^k=1/M$. Then suppose that we have a weighted sample $\{x_t^k,{w'}_t^k|k=1,\ldots,M\}$ from $p(x_t|y_{1:t}$). First generate an equally weighted sample by resampling with replacement M times to obtain $\{\tilde{x}_t^k|k=1,\ldots,M\}$ (giving an approximate random sample from $p(x_t|y_{1:t}$)). Note that each sample is independently drawn from $\sum_{i=1}^M {w'}_t^i\delta(x-x_t^i)$. Next propagate each particle forward according to the Markov process model by sampling $x_{t+1}^k\sim p(x_{t+1}|\tilde{x}_t^k),\ k=1,\ldots,M$ (giving an approximate random sample from $p(x_{t+1}|y_{1:t}$)). Then for each of the new particles, compute a weight $w_{t+1}^k=p(y_{t+1}|x_{t+1}^k$), and then a normalised weight ${w'}_{t+1}^k=w_{t+1}^k/\sum_i w_{t+1}^i$. \\

Sequential Importance Resampling (SIR) filters with transition prior probability distribution as importance function are commonly known as bootstrap filter. This choice is motivated by the facility of drawing particles and performing subsequent importance weight calculations. Here, $\pi(x_k| x_{0:k-1}, y_{0:k}) = p(x_k|x_{k-1})$ and the weights formula is now:

$$
w_k^{(i)} = w_{k-1}^{(i)} \frac{p(y_k|x_k^{(i)})p(x_k^{(i)}|x^{(i)}_{k-1})}{\pi(x_k^{(i)}|x^{(i)}_{0:k-1},y_{0:k})}= w_{k-1}^{(i)} p(y_k|x_k^{(i)})
$$ 


It is clear from our understanding of importance resampling that these weights are appropriate for representing a sample from $p(x_{t+1}|y_{1:t+1})$, and so the particles and weights can be propagated forward to the next time point. It is also clear that the average weight at each time gives an estimate of the marginal likelihood of the current data point given the data so far. So we define the conditional marginal of $y_t$:

$$ p^N_{\theta}(y_t | y_{1:t-1}) = \frac{1}{N} \sum_{k=1}^N w_t^k$$

and the conditional marginal $y_{1:T}$ over all the state space is:

$$ p^N_{\theta}(y_{0:T}) = p(y_1)\prod_{t=2}^T p(y_t | y_{1:t-1})$$

As $T$ is usually large, it is preferred to work with the log likelihoods:
\begin{align*}
\log p_{\theta}(y_{1:t}) &= \log(p_\theta(y_1)) + \sum_{t=2}^t \log p_\theta(y_t | y_{1:t-1}) \\
\log \hat{p}^N_{\theta}(y_{1:t}) &= \sum_{t=2}^t \log \left(\frac{1}{N} \sum_{k=1}^N w_t^{(t)} \right)
\end{align*}

\begin{algorithm}
\caption{Bootstrap Particle Filtering Algorithm (SIR)}\label{euclid}
\begin{algorithmic}[1]
\Procedure{Input}{$y_{1:T}$, $\theta$, N}
\For{i from 1 to N}
	\State Sample $x_1^{(i)}$ independently from $p(x_1)$
	\State Calculate weights $w_1^{(i)} = p(y_1 | x_1^{(i)})$
\EndFor{end}
\State $x^*_1 = \sum_{i=1}^N x_1^{(i)}.w_1^{(i)}$
\State Set $\hat{p}(y_1) = \frac{1}{N} \sum_{i=1}^N w_1^{(i)}$

\For{t from 1 to T}
	\For{i from 1 to N}
		\State Sample $j$ from 1:N with probabilities proportional to $\{w_{t-1}^{(1)},..., w_{t-1}^{(N)}\}$
		\State Sample $x_t^{(i)}$ from $p(x_t|x_{t-1})$
		\State Calculate weights $w_t^{(i)} = p(y_t|x_t^{(i)})$
	\EndFor{end}
	\State $x^*_t = \sum_{i=1}^N x_t^{(i)}.w_t^{(i)}$
	\State Set $\hat{p}(y_{1:t}) = \hat{p}(y_{1:t-1}) \left(\frac{1}{N} \sum_{i=1}^N w_t^{(i)} \right)$
\EndFor{end}
\\
\Return ($x^*_{1:T}$, $\hat{p}(y_{1:T})$)}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Again, from the importance resampling scheme, it should be reasonably clear that $p^N_{\theta}(y_{1:T})$ is a consistent estimator of $p_{\theta}(y_{1:T})$. It is much less obvious, but nevertheless true that this estimator is also unbiased. This result is the cornerstone of Particle MCMC models, especially for the particle marginal Metropolis-Hastings Algorithm  explained in the next section.

\section{Particle marginal Metropolis-Hastings Algorithm}

Before explaining in details how the Particle marginal Metropolis-Hastings Algorithm (PMMH) works, a more general context is presented. The Metropolis Hastings MCMC scheme is used to target $p(\theta| y) \propto p(y|\theta)p(\theta)$ with the ratio:

\begin{align*}
\min \left( 1, \frac{p(\theta^\star)}{p(\theta)} \times  \frac{q(\theta|\theta^\star)}{q(\theta^\star|\theta)} \times \frac{p({y}|\theta^\star)}{p({y}|\theta)} \right)
\end{align*}

where $q(\theta^\star|\theta)$ is the proposal density. As discussed before, in hidden Markov models, the marginal likelihood $p(y|\theta) = \int_{\mathbb{R}^T} p(y|x)p(x|\theta) dx$ is often intractable and the ratio is hard to compute. The simple likelihood-free scheme targets the full joint posterior $p(\theta,x|y)$. Usually the knowledge of the kernel $K(x_t|x_{t-1})$ makes $p(x|\theta)$ tractable. For instance, for a linear Gaussian process $x_{t+1} = \rho x_t + \tau \epsilon_{t+1}$, a path $x_{0:T}$ can be simulated as long as $\rho$, $\tau$ and $x_0$ are known quantities. The MH is built in two stages. First, a new $\theta^*$ is proposed from $q(\theta^\star|\theta)$. Then, $x^*$ is sampled from $p(x^\star|\theta^\star)$. The generated pair $(\theta^\star,x^\star)$ is accepted with the ratio:

\begin{align*}
\min \left( 1, \frac{p(\theta^\star)}{p(\theta)} \times  \frac{q(\theta|\theta^\star)}{q(\theta^\star|\theta)} \times \frac{p(y|{x}^\star,\theta^\star)}{p(y|{x},\theta)} \right)
\end{align*}
At each step, $x^*$ is consistent with $\theta^*$ because it was generated from $p(x^\star|\theta^\star)$. The problem of this approach is that the sampled $x^*$ may not be consistent with $y$. As $T$ grows, it becomes nearly impossible to iterate over all possible values of $x^\star$ to track $p(y|x^\star,\theta)$. This is the reason why $x^*$ should be sampled from $p(x^\star|\theta^\star,y)$. With the remark, the ratio now becomes:

\begin{align*}
 \min \left(1, \frac{p(\theta^\star)}{p(\theta)}   \frac{p({x}^\star|\theta^\star)}{p({x}|\theta)}   \frac{f(\theta|\theta^\star)}{f(\theta^\star|\theta)}   \frac{p(y|{x}^\star,\theta^\star)}{p(y|{x},\theta)}  \frac{p({x}|y,\theta)}{p({x}^\star|y,\theta^\star)} \right)
\end{align*}

Using the basic marginal likelihood identity of Chib (1995), the ratio is simplified to:

\begin{align*}
 \min \left(1, \frac{p(\theta^\star)}{p(\theta)}  \frac{p(y|\theta^\star)}{p(y|\theta)} \frac{f(\theta|\theta^\star)}{f(\theta^\star|\theta)} \right)
\end{align*}

It is now clear that a pseudo-marginal MCMC scheme for state space models can be derived by substituting $\hat{p}^N_{\theta}(y_{1:T})$, computed from a particle filter, in place of $p_{\theta}(y_{1:T})$. This turns out to be a simple special case of the particle marginal Metropolis-Hastings (PMMH) algorithm described in Andreiu et al (2010).

Remarkably $x$ is no more present and the ratio is exactly the same as the marginal scheme shown before. Indeed the ideal marginal scheme corresponds to PMMH when $N \rightarrow +\infty$. The likelihood-free scheme is obtained with just one particle in the filter. When $N$ is intermediate, the PMMH algorithm is a trade-off between the ideal and the likelihood-free schemes, but is always likelihood-free when one bootstrap particle filter is used.

The PMMH algorithm is depicted below.

\begin{algorithm}
\caption{Particle pseudo marginal Metropolis-Hastings Algorithm}\label{euclid}
\begin{algorithmic}[1]
\Procedure{Input}{$y_{1:T}$, a proposal distribution $q(\cdot|\cdot)$, the number of particles $N$, the number of MCMC steps $M$}

\State $\hat{p}^N_{\theta^{(0)}}(y_{1:T}), x^*_{1:T}^{(0)}$ $ \gets$ Call Bootstrap Particle Filter with ($y_{1:T}$, $\theta^{(0)}$, N)

\For{i from 1 to M}
	\State Sample $\theta'}$ from q(\theta|\theta^{(i-1)})$
	\State $\hat{p}^N_{\theta'}(y_{1:T}), x^*_{1:T}'$ $ \gets$ Call Bootstrap Particle Filter with ($y_{1:T}$, $\theta'$, N)
	\State With probability,
	
	$$\min \left\{1, \frac{q(\theta^{(i-1)}|\theta')\hat{p}_N(y_{1:T}|\theta')p(\theta')}{q(\theta'|\theta^{(i-1)})\hat{p}_N(y_{1:T}|\theta^{(i-1)})p(\theta^{(i-1)})}  \right\} $$
	
	\State Set $x^*_{1:T}^{(i)} \gets x^*_{1:T}',\theta^{(i-1)} \gets \theta', \hat{p}^N_{\theta^{(i)}}(y_{1:T}) \gets \hat{p}^N_{\theta'}(y_{1:T})$
	\State Otherwise $x^*_{1:T}^{(i)} \gets x^*_{1:T}^{(i-1)},\theta^{(i-1)} \gets \theta^{(i-1)}, \hat{p}^N_{\theta^{(i)}}(y_{1:T}) \gets \hat{p}^N_{\theta^{(i-1)}}(y_{1:T})$
	
\EndFor{end}
\\
\EndProcedure
\Return $(x^*_{1:T}^{(i)}, \theta^{(i)})_{i=1}^M$}

\end{algorithmic}
\end{algorithm}

\section{Heston}
\subsection{Simulation of Probability Densities}
By Ito calculus, and more precisely the Euler-Maruyama method, the Heston stochastic process can be discretized and results in:

\begin{align*}
S_t &= S_{t-1} + r S_{t-1} dt + \sqrt{V_{t-1}}S_{t-1} \sqrt{dt}Z_t^S \\
V_t &= V_{t-1} + \kappa(\theta - V_{t-1})dt + \sigma \sqrt{V_{t-1}} \sqrt{dt} Z_t^V
\end{align}
where the innovations $\{Z_t^S\}_{t \geq 0}$ and $\{Z_t^V\}_{t \geq 0}$ are standard normal random variables with correlation $\rho$. The generation is made simple by considering the Cholesky decomposition,

\begin{align*}
Z_t^S &= \phi_t^S \\
Z_t^V &= \rho \phi_t^V + \sqrt{1-\rho^2} \phi_t^V
\end{align}
where $\{\phi_t^S\}_{t \geq 0}$ and $\{\phi_t^S\}_{t \geq 0}$ are independent standard normal random variables.
%%%% http://math.nyu.edu/~atm262/fall06/compmethods/a1/nimalinmoodley.pdf p36


\section{Stochastic Volatility}
In this section, we introduce the standard stochastic volatility with Gaussian errors. Next, we consider different well-known extensions of the SV model. The first extension is a SV model with Student-t errors. In the second extension, we incorporate a leverage effect by modeling a correlation parameter between measurement and state errors. In the third extension, we implement a model that has both stochastic volatility and moving average errors.

\subsection{Simple SV Model}
The standard discrete-time stochastic volatility model for the returns $Y_n$ is defined as:
\begin{align*}
  X_{n+1} &=  \rho X_{n} + \sigma V_n \\
  Y_n &=  \beta \exp \left( \frac{X_n}{2} \right) W_n
\end{align*}
where  $\{V_n\}$,$\{W_n\}$ are two independent sequences of independent standard normal random variables. Let $\theta = (\rho, \sigma^2, \beta)$. Notice that the non-linearity of the models relies in the non-additive noise of the transition Kernel. $X_n$ is the unobserved log-volatility associated to the observed log-returns $Y_n$, $\sigma$ is the volatility of the log-volatility and $\rho$ is the persistence parameter. The condition $|\rho| < 1$  is imposed to have a stationary process with the initial condition $X_0 \sim \mathcal{N} \left(0, \frac{\sigma^2}{1-\rho^2} \right)$.

\subsection{SVt - Student-t innovations}
The first extension is a stochastic volatility model with ${W_n} \sim St(\nu)$ where $St$ stands for the Student-t distribution with $\nu > 2$. The conditional density becomes (pdf variable substitution):

$$p(y_n | x_n, Y_{n-1},  \theta) = \frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2}) \sqrt{(\nu-2)\pi}} \frac{1}{\sigma_n}\left( 1 + \frac{y_n^2}{\sigma_n^2 \nu}\right)^{-\frac{v+1}{2}}$$
where $\sigma_n = \beta \exp \left(\frac{x_n}{2} \right)$. We then follow the sampling steps as before.

\subsection{SVL - Stochastic Volatility Leverage}
In the second extension, we incorporate a leverage effect by letting $c$ denote the correlation between $V_n$ and $W_n$. Here, we use the fact that $V_n = c W_n + \sqrt{1-c^2} \Psi_n$ where $\Psi_n \sim N(0,1) :$

\begin{align*}
X_{n+1} &=  \rho X_{n} + \sigma \left(c W_n + \sqrt{1-c^2} B_n \right) \\
X_{n+1} &=  \rho X_{n} + \sigma \left(c Y_n \exp \left( - \frac{X_n}{2} - \log(\beta) \right)  + \sqrt{1-c^2} B_n \right)
\end{align*}

Notice that we need to sample the additional parameter $c$.

\subsection{SV-MA(1) - Moving Average}
We can also expand the plain stochastic volatility model by allowing the errors in the measurement equation to follow a moving average (MA) process of order $m$. This means that the errors in the measurement equation are no longer serially independent as for the plain SV model. Here, we choose a more simple specification and set $m = 1$. Hence, our model becomes:

\begin{align*}
Y_n &=  \beta \exp \left( \frac{X_n}{2} \right) W_n + \Psi \beta \exp \left( \frac{X_{n-1}}{2} \right) W_{n-1} \\
X_{n+1} &=  \rho X_{n} + \sigma V_n
\end{align*}
We ensure that the root of the characteristic polynomial assocaited with the MA coefficient $\Psi$ is outside the unit circle, $|\Psi$| \\

In the following, we will assume that a process $(X_t)_{t \in \mathbb{N}}$ is adapted to a filtration $(\mathcal{F}_t)_{t \in \mathbb{N}}$ which presents the accrual of information over time. We denote by $\mathcal{F}_t = \sigma \{X_s : s \leq t \}$ the $\sigma$-algebra generated by the history of $X$ up to time $t$. The corresponding filtration is then called the natural filtration.

$$Var(y_t | \mathcal{F}_{t-1}) = \exp(X_t) + \Psi^2 \exp(X_{t-1})$$
because $X_t$ is measurable with regard to $\mathcal{F}_{t-1}$. It turns out that the conditional variance of $y_t$ is varying through two channels. Estimating this model is straightfoward as again we only need to make small adjustments in the codes.

\subsection{SV-M}
Let's consider the population stochastic volatility in mean (SVM) model where $\exp(X_t /2)$ appears in both the conditional mean and the conditional volatility. We follow the same notation as before and define the SVM model as: 

$$y_t = \beta \exp\left(\frac{X_t}{2}\right) + \exp\left(\frac{X_t}{2}\right) + W_t, \text{ }, W_t \sim N(0,1)$$
where $X_t$ is ruled by the dynamics of a simple SV model. The conditional probability density of $y_t$ is $p(y_t | x_t, Y_{t-1}, \theta) \sim N(\beta \exp(x_t/2), exp(x_t))$.

\subsection{TFSV - Two Factors}
Finally, we estimate a two factor SV model. It is defined as:
\begin{align*}
  X_{n+1} &=  \rho_1 X_{n} + \sigma_2 V_n, \text{ } |\rho_1| < 1, V_n \sim N(0,1)\\
  Z_{n+1} &=  \rho_2 Z_{n} + \sigma_2 P_n, \text{ } |\rho_2| < 1, P_n \sim N(0,1) \\
  Y_n &=  \exp \left(\frac{\mu}{2} + \frac{X_n+Z_n}{2} \right) W_n, \text{ } |\rho_2| < 1, W_n \sim N(0,1)
\end{align*}
$\theta$ is enriched with the new parameters. Thus, we only need to modify the particle filter such that we drawa two sets of particles (one for $X_t$ and one for $Z_t$) instead of one.

\section{Model Comparison}
The output of the particle filter is an estimate of $p(y|\theta)$, with the unobserved states integrated out. The marginal likelihood for a model $\mathcal{M}$ is defined as:

$$p(Y_T | \mathcal{M}) = \int_{\theta} p(Y_T | \theta, \mathcal{M}) p(\theta | \mathcal{M}) d\theta$$

Gelfand and Dey (1994) proposed a very general estimate for this marginal likelihood:

$$\frac{1}{N} \sum_{i=1}^N \frac{g(\theta_i)}{p(Y_T | \theta_i) p(\theta_i)} \rightarrow \frac{1}{p(Y_T)}  \text{ as It}_{mcmc} \rightarrow +\infty$$
For this estimator to be consistent, $g(\theta_i)}$ must be thin-tailed relative to the denominator. For most cases, a multivariate Normal distribution $N(\theta^*, \Sigma^*)$ can be used, where $\theta^* = \frac{1}{N} \sum_{i=1}^N \theta^i$ and $\Sigma^* = \frac{1}{N-1} \sum_{i=1}^N \left(\theta^i - \theta^*\right)\left(\theta^i - \theta^*\right)^T$. The difficulty of this approach resides in the implementation. As a matter of fact, $p(Y_T | \theta)$ is usually very small as $T$ grows. The trick here is to consider the sum of the exponential of the logarithms and factorize by the maximum logarithm to avoid rounding errors. For example, when N = 3 and let assume that the log-terms on the LHS are equal to $-120$, $-121$ and $-122$ :

\begin{align*}
p(Y_T)^{-1} &= e^{-120} + e^{-121} + e^{-122} \\
- \log p(Y_T) &= \log (e^{-120} ( 1 + e^{-1} + e^{-2})) \\
 \log p(Y_T) &= 120 - \log ( 1 + e^{-1} + e^{-2})) \simeq 119.6
\end{align*}


When $p(Y_T | \mathcal{M_A})$ and $p(Y_T | \mathcal{M_B})$ have been estimated, Kass and Raftery (1995) suggest to use twice the logarithm of the Bayes factor for model comparison, $2 \log BF_{\mathcal{M_{AB}}}$. The evidence of $\mathcal{M_A}$ over $\mathcal{M_B}$ is based on a rule-of-thumb: 0 to 2 not worth more than a bare mention, 2 to 6 positive, 6 to 10 strong, and greater than 10 as very strong.

\section{Resampling}
Resampling is a key component of the Particle Filter. Different methods exist: stratified, systematic and residuals resampling. In practical applications, they are generally found to provide comparable results. Despite the lack of complete theoretical analysis of its behavior, systematic resampling is often preferred because it is the simplest method to implement. Randal Douc proved that residual and stratified resampling methods dominate the basic multinomial approach, in the sense of having lower conditional variance for all configurations of the weights.

\begin{table}[h]
\centering
\label{table1}
\begin{tabular}{|l|l|l|l|l|}
\hline
Resampling method     & Residual & Stratified & Systematic & Multinomial \\ \hline
Time (in seconds) & 18.90    & 0.62       & 0.63       & 1.87        \\ \hline
\end{tabular}
\caption{Time spent to resample 100K times 1000 weights}
\end{table}
The multinomial implementation is the MATLAB default version. According to Randal Douc and the performance results, the stratified resampling seems the most compelling method to use inside the particle filters. This part is critical because it can represent up to 50$\%$ of the total time spent in the filter.

\chapter{Trading Strategy}
\section{Statistical Arbitrage}
Statistical arbitrage conjectures statistical mis-pricings or price relationships that are true in expectation, in the long run when repeating a trading strategy. Statistical arbitrage is a heavily quantitative and computational approach to equity trading. It describes a variety of automated trading systems which commonly make use of data mining, statistical methods and artificial intelligence techniques. A popular strategy is pairs trade, in which stocks are put into pairs by fundamental or market-based similarities. When one stock in a pair outperforms the other, the poorer performing stock is bought long with the expectation that it will climb towards its outperforming partner, the other is sold short. This hedges risk from whole-market movements. This idea can be easily generalized to $n$ stocks or assets where an asset can be a sector index.

- The strategy is a mean-reverting strategy. Once the spread (define it) is far from its long-run equilibrium, enter position and unwind (timing is important)

\section{Bollinger Bands}
Bollinger Bands is a widely used technical volatility indicator which consists in placing volatility bands $\{Boll^+_t, Boll^-_t\}$ above and below the moving average prices $\{Ma_t\}$. Volatility is based on the standard deviation, which changes as volatility increases and decreases. The bands automatically widen when volatility increases and narrow when volatility decreases. They are calculated by:

\begin{align*}
Ma(t) &= \frac{1}{n}\sum_{j=1}^n p_j \text{ (SMA)} \\
Boll^\pm(t) &= Ma(t) \pm d * \sqrt{\frac{1}{n} \sum_{j=1}^n (p_j - Ma(t))}
\end{align}
where $n$ is the number of time periods in the moving average and $d$ is the number of standard deviations to shift the bollinger bands. The default values are $n = 20$ and $d = 2$. The moving average can be replaced by an exponential moving average that gives more weights to new values and increase the accuracy eventually.

\section{Strategy}
The investment strategy we aim at implementing is market neutral, thus we will hold a long and a short position both having the same value in local currency. This approach has the advantage of eliminating the market exposure. A typical trading strategy is made of three parts: selection of the suitable tuples satisfying some criterias like the cointegration, create trading signals based on define predefined investment decision rules and finally assess the performance of the strategy.

\subsection{Tuples selection}
Having in mind that increasing the size $n$ of the tuples leads to an combinatorial explosion, it is necessary to make some trade-offs. First, cointegration usually implies correlation but correlation doesn't always imply cointegration. Spurious regression is a very good example where the reverse is not true. However, a correlation test is usually much faster than for instance a Johansen cointegration test (cf. table 2).

\begin{table}[h]
\centering
\label{table1}
\begin{tabular}{|l|l|l|l|l|}
\hline
Test     & Correlation & Johansen & Aug. Dickey Fuller & Phillips-Perron \\ \hline
Time & 0.33 ms   & 19.08 ms      & 2.33 ms      & 3.04 ms        \\ \hline
\end{tabular}
\caption{Average time spent to test a bivariate time series $X_t = (x_{t1}, x_{t2})$}
\end{table}

A simple correlation test is used for pair trading. When $n \geq 3$, it is preferred to use the multiple correlation coefficient, better known as $R^2$. It can be computed using the vector $c = (r_{x1y}, r_{x2y},...,r_{xNy})^T$ of correlation $r_{xny}$ between the predictor variables $x_n$ and the target variable $y$, and the correlation matrix $R_{xx}$ of inter-correlations between predictor variables. It is given by $R^2 = c^T R_{xx}^{-1}c$ where $R_{xx}^{-1}$ is the inverse of the matrix

$$R_{xx} =\begin{pmatrix}
r_{x1x1}    & r_{x1x2} & ...  & r_{x1xn}  \\
r_{x2x1}    & \ddots &   & \vdots  \\
\vdots       &   & \ddots &   \\
r_{xnx1}    & \hdots &   & r_{xnxn} \\
\end{pmatrix} $$
One problem arises: the value of the coefficient depends on the order of the tuple. A regression of $y$ on $x$ and $z$ will in general have a different $R$ that will a regression of $z$ on $x$ and $y$. To convince ourselves, let $z$ be uncorrelated with both $x$ and $y$ while $x$ and $y$ are linearly related to each other. A regression of $z$ on $y$ and $x$ will yield a $R$ of zero, while a regression of $y$ on $x$ and $z$ will yield a strictly positive $R$. It means that the order inside a tuple has its importance, at least from a statistical point of view. It is much less obvious from a pure financial point of view. \\

For each tuple, a $R$ is computed. A threshold $R_{th}$ is then derived according to this empirical distribution. For every selected tuple, apply a triple Johansen, Dickey Fuller and Phillips-Perron test. If the tuple is integrated, form the spread and mark it as tradable.

\chapter{Performance Assessment}
In order to reduce risk in the strategies, it is interesting to open many trades inside a portfolio, all with a very short holding time, hoping to diversify the risk of each trade.


\section{Presentation of the dataset - Big introduction}
The dataset is composed ...
The sample period used starts in January 1990 and ends in March 2014 summing up to 8844 observations. Daily equity closing prices obtained from Bloomberg. The analysis covers all stocks in the SP500 index from the american stock markets. The proposed statistical arbitrage generated average excess returns of 12\% per year in out-of-samples simulations, Sharpe ratio of 1.70, low exposure to the equity market and relatively low volatility. Even in market crashes, it turns out that the strategy is still highly profitable, reinforcing the usefulness of cointegration in quantitative strategies.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\bibname} % Add an entry for the Bibliography in the Table of Contents
%\pagestyle{headings}
\bibliographystyle{abbrvnat} % set the bibliography style
\bibliography{bibtexfile} % generate the bibliography
%\bibliographystyle{abbrvnat} % <- Mistake in earlier version. After the bibliography is created it's too late to change the style.
% Pick a sensible bibliography style. 
% If like above you choose to use author-year style citations, you must choose a natbib-compatible bibliographystyle such as 'plainnat'. Abbrvnat is one of the few built-in styles of this type. You may find many more bibliography styles (*.bst files) online. You can also choose to write your bibliography manually instead of using BibTeX (This will take you longer, unless you plan to use the content of a BibTeX-generated *.bbl file as your starting point).
% When creating a BibTeX file using e.g. reference manager software, note that the entries may contain additional unwanted fields which you would then need to erase. For example, you probably don't want to include the URL of a journal paper in your bibliography.


%\cleardoublepage \fancyhead[L]{APPENDIX}
\appendix % This line declares that you are starting the appendix.
% If you want a single Appendix and want it to be called Appendix instead of Appendix A, the following should work:
%\setcounter{secnumdepth}{-1} %This turns off automatic chapter numbering
%\chapter{Appendix}


\end{document}